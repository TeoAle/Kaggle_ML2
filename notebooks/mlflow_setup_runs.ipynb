{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src/')\n",
    "from dataloader import *\n",
    "from logging_utils import *\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import optuna\n",
    "\n",
    "# start mlflow server from terminal: $mlflow server mlflow server --host 127.0.0.1 --port 8080\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data/'\n",
    "\n",
    "df = load_train_df(\n",
    "    PATH,\n",
    "    decode_dummies=True,\n",
    "    add_geo_features=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Cover_Type']\n",
    "X = df.drop(['Cover_Type'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8673941798941799\n"
     ]
    }
   ],
   "source": [
    "test_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "test_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = test_model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "def objective(trial, experiment_id):\n",
    "    with mlflow.start_run(experiment_id=experiment_id, nested=True):\n",
    "        # Define hyperparameters\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 100),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 8),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "        }\n",
    "\n",
    "        model = RandomForestClassifier(**params)\n",
    "\n",
    "        # TODO THE FOLLOWING IS NOT VERY SOUND: CV SCORE AND CMATRIX ON DIFF SPLITS\n",
    "        acc = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "        f1 = cross_val_score(model, X_train, y_train, cv=3, scoring='f1_macro').mean()\n",
    "        \n",
    "        metrics = {\n",
    "            \"accuracy\": acc,\n",
    "            \"f1_macro\": f1,\n",
    "\n",
    "        }\n",
    "\n",
    "        y_pred = cross_val_predict(model, X_train, y_train)\n",
    "        \n",
    "        # fig = plot_confusion_matrix(y, y_pred)\n",
    "        # mlflow.log_figure(fig, \"confusion_matrix.png\")\n",
    "\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        # NOTE model is not logged to mlflow: eval if worth and then save it\n",
    "\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 value: 0.6895667989417991\n"
     ]
    }
   ],
   "source": [
    "# NOTE: create a new experiment whenever the data changes\n",
    "# create mlflow experiment\n",
    "confirm = input(\"Launch new mlflow experiment? (y/n)\")\n",
    "if confirm == \"y\":\n",
    "    experiment_id = get_or_create_experiment(\"First Optuna Experiment\")\n",
    "    mlflow.set_experiment(experiment_id=experiment_id)\n",
    "\n",
    "    run_name = input(\"Enter run name: \")\n",
    "    with mlflow.start_run(\n",
    "        experiment_id=experiment_id,\n",
    "        run_name=run_name\n",
    "        ):\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, experiment_id),\n",
    "            n_trials=3,\n",
    "            callbacks=[champion_callback]\n",
    "            )\n",
    "\n",
    "        mlflow.log_params(study.best_params)\n",
    "\n",
    "        best_model = RandomForestClassifier(**study.best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"f1_macro\": f1_score(y_test, y_pred, average='macro'),\n",
    "        }\n",
    "\n",
    "        cm = plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        mlflow.log_figure(cm, \"confusion_matrix.png\")\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.sklearn.log_model(best_model, \"best_model\")\n",
    "        mlflow.set_tag(\"mlflow.note.content\", \"This is a test run\")\n",
    "\n",
    "else:\n",
    "    print(\"No new experiment created\")\n",
    "    sys.exit(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
