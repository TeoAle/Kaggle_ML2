{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65854a65",
   "metadata": {},
   "source": [
    "## MLflow with Optuna: Hyperparameter Optimization and Tracking\n",
    "\n",
    "A critical part of building production-grade models is ensuring that a given model's parameters are selected to create the best inference set possible. However, the sheer number of combinations and their resultant metrics can become overwhelming to track manually. That's where tools like MLflow and Optuna come into play.\n",
    "\n",
    "### Objective: \n",
    "In this notebook, you'll learn how to integrate MLflow with Optuna for hyperparameter optimization. We'll guide you through the process of:\n",
    "\n",
    "* Setting up your environment with MLflow tracking.\n",
    "* Generating our training and evaluation data sets.\n",
    "* Defining a partial function that fits a machine learning model.\n",
    "* Using Optuna for hyperparameter tuning.\n",
    "* Leveraging child runs within MLflow to keep track of each iteration during the hyperparameter tuning process.\n",
    "\n",
    "### Why Optuna?\n",
    "Optuna is an open-source hyperparameter optimization framework in Python. It provides an efficient approach to searching over hyperparameters, incorporating the latest research and techniques. With its integration into MLflow, every trial can be systematically recorded.\n",
    "\n",
    "### Child Runs in MLflow:\n",
    "One of the core features we will be emphasizing is the concept of 'child runs' in MLflow. When performing hyperparameter tuning, each iteration (or trial) in Optuna can be considered a 'child run'. This allows us to group all the runs under one primary 'parent run', ensuring that the MLflow UI remains organized and interpretable. Each child run will track the specific hyperparameters used and the resulting metrics, providing a consolidated view of the entire optimization process.\n",
    "\n",
    "### What's Ahead?\n",
    "\n",
    "**Data Preparation**: We'll start by loading and preprocessing our dataset.\n",
    "\n",
    "**Model Definition**: Defining a machine learning model that we aim to optimize.\n",
    "\n",
    "**Optuna Study**: Setting up an Optuna study to find the best hyperparameters for our model.\n",
    "\n",
    "**MLflow Integration**: Tracking each Optuna trial as a child run in MLflow.\n",
    "\n",
    "**Analysis**: Reviewing the tracked results in the MLflow UI.\n",
    "\n",
    "By the end of this notebook, you'll have hands-on experience in setting up an advanced hyperparameter tuning workflow, emphasizing best practices and clean organization using MLflow and Optuna. \n",
    "\n",
    "**Let's dive in!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db63d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6162d0f3",
   "metadata": {},
   "source": [
    "### Configure the tracking server uri\n",
    "\n",
    "Depending on where you are running this notebook, your configuration may vary for how you initialize the interface with the MLflow Tracking Server. \n",
    "\n",
    "For this example, we're using a locally running tracking server, but other options are available (The easiest is to use the free managed service within [Databricks Community Edition](https://community.cloud.databricks.com/)). \n",
    "\n",
    "Please see [the guide to running notebooks here](https://www.mlflow.org/docs/latest/getting-started/running-notebooks/index.html) for more information on setting the tracking server uri and configuring access to either managed or self-managed MLflow tracking servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f72ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: review the links mentioned above for guidance on connecting to a managed tracking server, such as the free Databricks Community Edition\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9ede21",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = get_or_create_experiment(\"Apples Demand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "074fd487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current active MLflow experiment\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = df.drop(columns=[\"date\", \"demand\"])\n",
    "y = df[\"demand\"]\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\n",
    "dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "dvalid = xgb.DMatrix(valid_x, label=valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884d226",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning and Model Training using Optuna and MLflow\n",
    "\n",
    "The `objective` function serves as the core of our hyperparameter tuning process using Optuna. Additionally, it trains an XGBoost model using the selected hyperparameters and logs metrics and parameters to MLflow.\n",
    "\n",
    "##### MLflow Nested Runs\n",
    "\n",
    "The function starts a new nested run in MLflow. Nested runs are useful for organizing hyperparameter tuning experiments as they allow you to group individual runs under a parent run.\n",
    "\n",
    "##### Defining Hyperparameters\n",
    "\n",
    "Optuna's `trial.suggest_*` methods are used to define a range of possible values for hyperparameters. Here's what each hyperparameter does:\n",
    "\n",
    "- `objective` and `eval_metric`: Define the loss function and evaluation metric.\n",
    "- `booster`: Type of boosting to be used (`gbtree`, `gblinear`, or `dart`).\n",
    "- `lambda` and `alpha`: Regularization parameters.\n",
    "- Additional parameters like `max_depth`, `eta`, and `gamma` are specific to tree-based models (`gbtree` and `dart`).\n",
    "\n",
    "##### Model Training\n",
    "\n",
    "An XGBoost model is trained using the chosen hyperparameters and the preprocessed training dataset (`dtrain`). Predictions are made on the validation set (`dvalid`), and the mean squared error (`mse`) is calculated.\n",
    "\n",
    "##### Logging with MLflow\n",
    "\n",
    "All the selected hyperparameters and metrics (`mse` and `rmse`) are logged to MLflow for later analysis and comparison.\n",
    "\n",
    "- `mlflow.log_params`: Logs the hyperparameters.\n",
    "- `mlflow.log_metric`: Logs the metrics.\n",
    "\n",
    "##### Why This Function is Important\n",
    "\n",
    "- **Automated Tuning**: Optuna automates the process of finding the best hyperparameters.\n",
    "- **Experiment Tracking**: MLflow allows us to keep track of each run's hyperparameters and performance metrics, making it easier to analyze, compare, and reproduce experiments later.\n",
    "\n",
    "In the next step, this objective function will be used by Optuna to find the optimal set of hyperparameters for our XGBoost model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb722d7",
   "metadata": {},
   "source": [
    "#### Housekeeping: Streamlining Logging for Optuna Trials\n",
    "\n",
    "As we embark on our hyperparameter tuning journey with Optuna, it's essential to understand that the process can generate a multitude of runs. In fact, so many that the standard output (stdout) from the default logger can quickly become inundated, producing pages upon pages of log reports. \n",
    "\n",
    "While the verbosity of the default logging configuration is undeniably valuable during the code development phase, initiating a full-scale trial can result in an overwhelming amount of information. Considering this, logging every single detail to stdout becomes less practical, especially when we have dedicated tools like MLflow to meticulously track our experiments.\n",
    "\n",
    "To strike a balance, we'll utilize callbacks to tailor our logging behavior.\n",
    "\n",
    "##### Implementing a Logging Callback:\n",
    "\n",
    "The callback we're about to introduce will modify the default reporting behavior. Instead of logging every trial, we'll only receive updates when a new hyperparameter combination yields an improvement over the best metric value recorded thus far.\n",
    "\n",
    "This approach offers two salient benefits:\n",
    "\n",
    "1. **Enhanced Readability**: By filtering out the extensive log details and focusing only on the trials that show improvement, we can gauge the efficacy of our hyperparameter search. For instance, if we observe a diminishing frequency of 'best result' reports early on, it might suggest that fewer iterations would suffice to pinpoint an optimal hyperparameter set. On the other hand, a consistent rate of improvement might indicate that our feature set requires further refinement.\n",
    "   \n",
    "2. **Progress Indicators**: Especially pertinent for extensive trials that span hours or even days, receiving periodic updates provides assurance that the process is still in motion. These 'heartbeat' notifications affirm that our system is diligently at work, even if it's not flooding stdout with every minute detail.\n",
    "\n",
    "Moreover, MLflow's user interface (UI) complements this strategy. As each trial concludes, MLflow logs the child run, making it accessible under the umbrella of the parent run.\n",
    "\n",
    "In the ensuing code, we:\n",
    "\n",
    "1. Adjust Optuna's logging level to report only errors, ensuring a decluttered stdout.\n",
    "2. Define a `champion_callback` function, tailored to log only when a trial surpasses the previously recorded best metric.\n",
    "\n",
    "Let's dive into the implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59065827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# override Optuna's default logging to ERROR only\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# define a logging callback that will report on only new challenger parameter configurations if a\n",
    "# trial has usurped the state of 'best conditions'\n",
    "\n",
    "\n",
    "def champion_callback(study, frozen_trial):\n",
    "    \"\"\"\n",
    "    Logging callback that will report when a new trial iteration improves upon existing\n",
    "    best trial values.\n",
    "\n",
    "    Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "    workers or agents.\n",
    "    The race conditions with file system state management for distributed trials will render\n",
    "    inconsistent values with this callback.\n",
    "    \"\"\"\n",
    "\n",
    "    winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "    if study.best_value and winner != study.best_value:\n",
    "        study.set_user_attr(\"winner\", study.best_value)\n",
    "        if winner:\n",
    "            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "            print(\n",
    "                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "                f\"{improvement_percent: .4f}% improvement\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5e6ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Define hyperparameters\n",
    "        params = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        }\n",
    "\n",
    "        if params[\"booster\"] == \"gbtree\" or params[\"booster\"] == \"dart\":\n",
    "            params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
    "            params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "            params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "            params[\"grow_policy\"] = trial.suggest_categorical(\n",
    "                \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "            )\n",
    "\n",
    "        # Train XGBoost model\n",
    "        bst = xgb.train(params, dtrain)\n",
    "        preds = bst.predict(dvalid)\n",
    "        error = mean_squared_error(valid_y, preds)\n",
    "\n",
    "        # Log to MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"mse\", error)\n",
    "        mlflow.log_metric(\"rmse\", math.sqrt(error))\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d228bc",
   "metadata": {},
   "source": [
    "#### Orchestrating Hyperparameter Tuning, Model Training, and Logging with MLflow\n",
    "\n",
    "This section of the code serves as the orchestration layer, bringing together Optuna for hyperparameter tuning and MLflow for experiment tracking. \n",
    "\n",
    "##### Initiating Parent Run\n",
    "\n",
    "We begin by starting a parent MLflow run with the name \"Best Run\". All subsequent operations, including Optuna's trials, are nested under this parent run, providing a structured way to organize our experiments.\n",
    "\n",
    "##### Hyperparameter Tuning with Optuna\n",
    "\n",
    "- `study = optuna.create_study(direction='minimize')`: We create an Optuna study object aiming to minimize our objective function.\n",
    "- `study.optimize(objective, n_trials=10)`: The `objective` function is optimized over 10 trials.\n",
    "\n",
    "##### Logging Best Parameters and Metrics\n",
    "\n",
    "After Optuna finds the best hyperparameters, we log these, along with the best mean squared error (`mse`) and root mean squared error (`rmse`), to MLflow.\n",
    "\n",
    "##### Logging Additional Metadata\n",
    "\n",
    "Using `mlflow.set_tags`, we log additional metadata like the project name, optimization engine, model family, and feature set version. This helps in better categorizing and understanding the context of the model run.\n",
    "\n",
    "##### Model Training and Artifact Logging\n",
    "\n",
    "- We train an XGBoost model using the best hyperparameters.\n",
    "- Various plots—correlation with demand, feature importance, and residuals—are generated and logged as artifacts in MLflow.\n",
    "  \n",
    "##### Model Serialization and Logging\n",
    "\n",
    "Finally, the trained model is logged to MLflow using `mlflow.xgboost.log_model`, along with an example input and additional metadata. The model is stored in a specified artifact path and its URI is retrieved.\n",
    "\n",
    "##### Why This Block is Crucial\n",
    "\n",
    "- **End-to-End Workflow**: This code block represents an end-to-end machine learning workflow, from hyperparameter tuning to model evaluation and logging.\n",
    "- **Reproducibility**: All details about the model, including hyperparameters, metrics, and visual diagnostics, are logged, ensuring that the experiment is fully reproducible.\n",
    "- **Analysis and Comparison**: With all data logged in MLflow, it becomes easier to analyze the performance of various runs and choose the best model for deployment.\n",
    "\n",
    "In the next steps, we'll explore how to retrieve and use the logged model for inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc0923",
   "metadata": {},
   "source": [
    "#### Setting a Descriptive Name for the Model Run\n",
    "\n",
    "Before proceeding with model training and hyperparameter tuning, it's beneficial to assign a descriptive name to our MLflow run. This name serves as a human-readable identifier, making it easier to track, compare, and analyze different runs.\n",
    "\n",
    "##### The Importance of Naming Runs:\n",
    "\n",
    "- **Reference by Name**: While MLflow provides unique identifying keys like `run_id` for each run, having a descriptive name allows for more intuitive referencing, especially when using particular APIs and navigating the MLflow UI.\n",
    "  \n",
    "- **Clarity and Context**: A well-chosen run name can provide context about the hypothesis being tested or the specific modifications made, aiding in understanding the purpose and rationale of a particular run.\n",
    "  \n",
    "- **Automatic Naming**: If you don't specify a run name, MLflow will generate a unique fun name for you. However, this might lack the context and clarity of a manually chosen name.\n",
    "\n",
    "##### Best Practices:\n",
    "\n",
    "When naming your runs, consider the following:\n",
    "\n",
    "1. **Relevance to Code Changes**: The name should reflect any code or parameter modifications made for that run.\n",
    "2. **Iterative Runs**: If you're executing multiple runs iteratively, it's a good idea to update the run name for each iteration to avoid confusion.\n",
    "\n",
    "In the subsequent steps, we will set a name for our parent run. Remember, if you execute the model training multiple times, consider updating the run name for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5303b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"first_attempt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81f90731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trial 0 achieved value: 1593256.879424474\n",
      "Trial 1 achieved value: 1593250.8071099266 with  0.0004% improvement\n",
      "Trial 2 achieved value: 30990.735000917906 with  5041.0552% improvement\n",
      "Trial 5 achieved value: 22804.947010998963 with  35.8948% improvement\n",
      "Trial 7 achieved value: 18232.507769997483 with  25.0785% improvement\n",
      "Trial 10 achieved value: 15670.64645523901 with  16.3482% improvement\n",
      "Trial 11 achieved value: 15561.843005727616 with  0.6992% improvement\n",
      "Trial 21 achieved value: 15144.954353687495 with  2.7527% improvement\n",
      "Trial 23 achieved value: 14846.71981618512 with  2.0088% improvement\n",
      "Trial 55 achieved value: 14570.287261018764 with  1.8972% improvement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamin.wilson/repos/mlflow-fork/mlflow/mlflow/models/signature.py:333: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  input_schema = _infer_schema(input_ex)\n",
      "/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Initiate the parent run and call the hyperparameter tuning child run logic\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):\n",
    "    # Initialize the Optuna study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    # Execute the hyperparameter optimization trials.\n",
    "    # Note the addition of the `champion_callback` inclusion to control our logging\n",
    "    study.optimize(objective, n_trials=500, callbacks=[champion_callback])\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_mse\", study.best_value)\n",
    "    mlflow.log_metric(\"best_rmse\", math.sqrt(study.best_value))\n",
    "\n",
    "    # Log tags\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            \"project\": \"Apple Demand Project\",\n",
    "            \"optimizer_engine\": \"optuna\",\n",
    "            \"model_family\": \"xgboost\",\n",
    "            \"feature_set_version\": 1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log a fit model instance\n",
    "    model = xgb.train(study.best_params, dtrain)\n",
    "\n",
    "    # Log the correlation plot\n",
    "    mlflow.log_figure(figure=correlation_plot, artifact_file=\"correlation_plot.png\")\n",
    "\n",
    "    # Log the feature importances plot\n",
    "    importances = plot_feature_importance(model, booster=study.best_params.get(\"booster\"))\n",
    "    mlflow.log_figure(figure=importances, artifact_file=\"feature_importances.png\")\n",
    "\n",
    "    # Log the residuals plot\n",
    "    residuals = plot_residuals(model, dvalid, valid_y)\n",
    "    mlflow.log_figure(figure=residuals, artifact_file=\"residuals.png\")\n",
    "\n",
    "    artifact_path = \"model\"\n",
    "\n",
    "    mlflow.xgboost.log_model(\n",
    "        xgb_model=model,\n",
    "        artifact_path=artifact_path,\n",
    "        input_example=train_x.iloc[[0]],\n",
    "        model_format=\"ubj\",\n",
    "        metadata={\"model_data_version\": 1},\n",
    "    )\n",
    "\n",
    "    # Get the logged model uri so that we can load it from the artifact store\n",
    "    model_uri = mlflow.get_artifact_uri(artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de89c4",
   "metadata": {},
   "source": [
    "#### Understanding the Artifact URI in MLflow\n",
    "\n",
    "The output 'mlflow-artifacts:/908436739760555869/c8d64ce51f754eb698a3c09239bcdcee/artifacts/model' represents a unique Uniform Resource Identifier (URI) for the trained model artifacts within MLflow. This URI is a crucial component of MLflow's architecture, and here's why:\n",
    "\n",
    "##### Simplified Access to Model Artifacts\n",
    "\n",
    "The `model_uri` abstracts away the underlying storage details, providing a consistent and straightforward way to reference model artifacts, regardless of where they are stored. Whether your artifacts are on a local filesystem, in a cloud storage bucket, or on a network mount, the URI remains a consistent reference point.\n",
    "\n",
    "##### Abstraction of Storage Details\n",
    "\n",
    "MLflow is designed to be storage-agnostic. This means that while you might switch the backend storage from, say, a local directory to an Amazon S3 bucket, the way you interact with MLflow remains consistent. The URI ensures that you don't need to know the specifics of the storage backend; you only need to reference the model's URI.\n",
    "\n",
    "##### Associated Information and Metadata\n",
    "\n",
    "Beyond just the model files, the URI provides access to associated metadata, the model artifact, and other logged artifacts (files and images). This ensures that you have a comprehensive set of information about the model, aiding in reproducibility, analysis, and deployment.\n",
    "\n",
    "##### In Summary\n",
    "\n",
    "The `model_uri` serves as a consistent, abstracted reference to your model and its associated data. It simplifies interactions with MLflow, ensuring that users don't need to worry about the specifics of underlying storage mechanisms and can focus on the machine learning workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20f2e23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlflow-artifacts:/908436739760555869/c28196b19e1843bca7e22f07d796e740/artifacts/model'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9239b0",
   "metadata": {},
   "source": [
    "#### Loading the Trained Model with MLflow\n",
    "\n",
    "With the line:\n",
    "\n",
    "```python\n",
    "loaded = mlflow.xgboost.load_model(model_uri)\n",
    "```\n",
    "we're leveraging MLflow's native model loader for XGBoost. Instead of using the generic pyfunc loader, which provides a universal Python function interface for models, we're using the XGBoost-specific loader.\n",
    "\n",
    "##### Benefits of Native Loading:\n",
    "\n",
    "- **Fidelity**: Loading the model using the native loader ensures that you're working with the exact same model object as it was during training. This means all nuances, specifics, and intricacies of the original model are preserved.\n",
    "\n",
    "- **Functionality**: With the native model object in hand, you can utilize all of its inherent methods and properties. This allows for more flexibility, especially when you need advanced features or fine-grained control during inference.\n",
    "\n",
    "- **Performance**: Using the native model object might offer performance benefits, especially when performing batch inference or deploying the model in environments optimized for the specific machine learning framework.\n",
    "\n",
    "In essence, by loading the model natively, we ensure maximum compatibility and functionality, allowing for a seamless transition from training to inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27f1c988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4d21b624d54d0d88bdae20765636c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded = mlflow.xgboost.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a1121a",
   "metadata": {},
   "source": [
    "#### Example: Batch Inference Using the Loaded Model\n",
    "\n",
    "After loading the model natively, performing batch inference is straightforward. \n",
    "\n",
    "In the following cell, we're going to perform a prediction based on the entire source feature set. \n",
    "Although doing an inference action on the entire training and validation dataset features is of very limited utility in a real-world application, we'll use our generated synthetic data here to illustrate using the native model for inference. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f0398",
   "metadata": {},
   "source": [
    "#### Performing Batch Inference and Augmenting Data\n",
    "\n",
    "In this section, we're taking our entire dataset and performing batch inference using our loaded XGBoost model. We'll then append these predictions back into our original dataset to compare, analyze, or further process.\n",
    "\n",
    "##### Steps Explained:\n",
    "\n",
    "1. **Creating a DMatrix**: `batch_dmatrix = xgb.DMatrix(X)`: We first convert our features (`X`) into XGBoost's optimized DMatrix format. This data structure is specifically designed for efficiency and speed in XGBoost.\n",
    "\n",
    "2. **Predictions**: `inference = loaded.predict(batch_dmatrix)`: Using the previously loaded model (`loaded`), we perform batch inference on the entire dataset.\n",
    "\n",
    "3. **Creating a New DataFrame**: `infer_df = df.copy()`: We create a copy of the original DataFrame to ensure that we're not modifying our original data.\n",
    "\n",
    "4. **Appending Predictions**: `infer_df[\"predicted_demand\"] = inference`: The predictions are then added as a new column, `predicted_demand`, to this DataFrame.\n",
    "\n",
    "##### Best Practices:\n",
    "\n",
    "- **Always Copy Data**: When augmenting or modifying datasets, it's generally a good idea to work with a copy. This ensures that the original data remains unchanged, preserving data integrity.\n",
    "\n",
    "- **Batch Inference**: When predicting on large datasets, using batch inference (as opposed to individual predictions) can offer significant speed improvements.\n",
    "\n",
    "- **DMatrix Conversion**: While converting to DMatrix might seem like an extra step, it's crucial for performance when working with XGBoost. It ensures that predictions are made as quickly as possible.\n",
    "\n",
    "In the subsequent steps, we can further analyze the differences between the actual demand and our model's predicted demand, potentially visualizing the results or calculating performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bacbb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dmatrix = xgb.DMatrix(X)\n",
    "\n",
    "inference = loaded.predict(batch_dmatrix)\n",
    "\n",
    "infer_df = df.copy()\n",
    "\n",
    "infer_df[\"predicted_demand\"] = inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd199e75",
   "metadata": {},
   "source": [
    "#### Visualizing the Augmented DataFrame\n",
    "\n",
    "Below, we display the `infer_df` DataFrame. This augmented dataset now includes both the actual demand (`demand`) and the model's predictions (`predicted_demand`). By examining this table, we can get a quick sense of how well our model's predictions align with the actual demand values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6310a692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>average_temperature</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>weekend</th>\n",
       "      <th>holiday</th>\n",
       "      <th>price_per_kg</th>\n",
       "      <th>promo</th>\n",
       "      <th>demand</th>\n",
       "      <th>previous_days_demand</th>\n",
       "      <th>competitor_price_per_kg</th>\n",
       "      <th>marketing_intensity</th>\n",
       "      <th>predicted_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-14 11:52:20.662955</td>\n",
       "      <td>30.584727</td>\n",
       "      <td>1.199291</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.726258</td>\n",
       "      <td>0</td>\n",
       "      <td>851.375336</td>\n",
       "      <td>851.276659</td>\n",
       "      <td>1.935346</td>\n",
       "      <td>0.098677</td>\n",
       "      <td>953.708496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-15 11:52:20.662954</td>\n",
       "      <td>15.465069</td>\n",
       "      <td>1.037626</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0</td>\n",
       "      <td>906.855943</td>\n",
       "      <td>851.276659</td>\n",
       "      <td>2.344720</td>\n",
       "      <td>0.019318</td>\n",
       "      <td>1013.409973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-16 11:52:20.662954</td>\n",
       "      <td>10.786525</td>\n",
       "      <td>5.656089</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.513328</td>\n",
       "      <td>0</td>\n",
       "      <td>1108.304909</td>\n",
       "      <td>906.836626</td>\n",
       "      <td>0.998803</td>\n",
       "      <td>0.409485</td>\n",
       "      <td>1152.382446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-17 11:52:20.662953</td>\n",
       "      <td>23.648154</td>\n",
       "      <td>12.030937</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.839225</td>\n",
       "      <td>0</td>\n",
       "      <td>1099.833810</td>\n",
       "      <td>1157.895424</td>\n",
       "      <td>0.761740</td>\n",
       "      <td>0.872803</td>\n",
       "      <td>1352.879272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-18 11:52:20.662952</td>\n",
       "      <td>13.861391</td>\n",
       "      <td>4.303812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.531772</td>\n",
       "      <td>0</td>\n",
       "      <td>983.949061</td>\n",
       "      <td>1148.961007</td>\n",
       "      <td>2.123436</td>\n",
       "      <td>0.820779</td>\n",
       "      <td>1121.233032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>2023-09-18 11:52:20.659592</td>\n",
       "      <td>21.643051</td>\n",
       "      <td>3.821656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.391010</td>\n",
       "      <td>0</td>\n",
       "      <td>1140.210762</td>\n",
       "      <td>1563.064082</td>\n",
       "      <td>1.504432</td>\n",
       "      <td>0.756489</td>\n",
       "      <td>1070.676636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>2023-09-19 11:52:20.659591</td>\n",
       "      <td>13.808813</td>\n",
       "      <td>1.080603</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898693</td>\n",
       "      <td>0</td>\n",
       "      <td>1285.149505</td>\n",
       "      <td>1189.454273</td>\n",
       "      <td>1.343586</td>\n",
       "      <td>0.742145</td>\n",
       "      <td>1156.580688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>2023-09-20 11:52:20.659590</td>\n",
       "      <td>11.698227</td>\n",
       "      <td>1.911000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.839860</td>\n",
       "      <td>0</td>\n",
       "      <td>965.171368</td>\n",
       "      <td>1284.407359</td>\n",
       "      <td>2.771896</td>\n",
       "      <td>0.742145</td>\n",
       "      <td>1086.527710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>2023-09-21 11:52:20.659589</td>\n",
       "      <td>18.052081</td>\n",
       "      <td>1.000521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.188440</td>\n",
       "      <td>0</td>\n",
       "      <td>1368.369501</td>\n",
       "      <td>1014.429223</td>\n",
       "      <td>2.564075</td>\n",
       "      <td>0.742145</td>\n",
       "      <td>1085.064087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>2023-09-22 11:52:20.659584</td>\n",
       "      <td>17.017294</td>\n",
       "      <td>0.650213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.131694</td>\n",
       "      <td>0</td>\n",
       "      <td>1261.301286</td>\n",
       "      <td>1367.627356</td>\n",
       "      <td>0.785727</td>\n",
       "      <td>0.833140</td>\n",
       "      <td>1047.954102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  average_temperature   rainfall  weekend   \n",
       "0    2010-01-14 11:52:20.662955            30.584727   1.199291        0  \\\n",
       "1    2010-01-15 11:52:20.662954            15.465069   1.037626        0   \n",
       "2    2010-01-16 11:52:20.662954            10.786525   5.656089        1   \n",
       "3    2010-01-17 11:52:20.662953            23.648154  12.030937        1   \n",
       "4    2010-01-18 11:52:20.662952            13.861391   4.303812        0   \n",
       "...                         ...                  ...        ...      ...   \n",
       "4995 2023-09-18 11:52:20.659592            21.643051   3.821656        0   \n",
       "4996 2023-09-19 11:52:20.659591            13.808813   1.080603        0   \n",
       "4997 2023-09-20 11:52:20.659590            11.698227   1.911000        0   \n",
       "4998 2023-09-21 11:52:20.659589            18.052081   1.000521        0   \n",
       "4999 2023-09-22 11:52:20.659584            17.017294   0.650213        0   \n",
       "\n",
       "      holiday  price_per_kg  promo       demand  previous_days_demand   \n",
       "0           0      1.726258      0   851.375336            851.276659  \\\n",
       "1           0      0.576471      0   906.855943            851.276659   \n",
       "2           0      2.513328      0  1108.304909            906.836626   \n",
       "3           0      1.839225      0  1099.833810           1157.895424   \n",
       "4           0      1.531772      0   983.949061           1148.961007   \n",
       "...       ...           ...    ...          ...                   ...   \n",
       "4995        0      2.391010      0  1140.210762           1563.064082   \n",
       "4996        1      0.898693      0  1285.149505           1189.454273   \n",
       "4997        0      2.839860      0   965.171368           1284.407359   \n",
       "4998        0      1.188440      0  1368.369501           1014.429223   \n",
       "4999        0      2.131694      0  1261.301286           1367.627356   \n",
       "\n",
       "      competitor_price_per_kg  marketing_intensity  predicted_demand  \n",
       "0                    1.935346             0.098677        953.708496  \n",
       "1                    2.344720             0.019318       1013.409973  \n",
       "2                    0.998803             0.409485       1152.382446  \n",
       "3                    0.761740             0.872803       1352.879272  \n",
       "4                    2.123436             0.820779       1121.233032  \n",
       "...                       ...                  ...               ...  \n",
       "4995                 1.504432             0.756489       1070.676636  \n",
       "4996                 1.343586             0.742145       1156.580688  \n",
       "4997                 2.771896             0.742145       1086.527710  \n",
       "4998                 2.564075             0.742145       1085.064087  \n",
       "4999                 0.785727             0.833140       1047.954102  \n",
       "\n",
       "[5000 rows x 12 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac6584e",
   "metadata": {},
   "source": [
    "#### Wrapping Up: Reflecting on Our Comprehensive Machine Learning Workflow\n",
    "\n",
    "Throughout this guide, we embarked on a detailed exploration of an end-to-end machine learning workflow. We began with data preprocessing, delved deeply into hyperparameter tuning with Optuna, leveraged MLflow for structured experiment tracking, and concluded with batch inference. \n",
    "\n",
    "##### Key Takeaways:\n",
    "\n",
    "- **Hyperparameter Tuning with Optuna**: We harnessed the power of Optuna to systematically search for the best hyperparameters for our XGBoost model, aiming to optimize its performance.\n",
    "\n",
    "- **Structured Experiment Tracking with MLflow**: MLflow's capabilities shone through as we logged experiments, metrics, parameters, and artifacts. We also explored the benefits of nested child runs, allowing us to logically group and structure our experiment iterations.\n",
    "\n",
    "- **Model Interpretation**: Various plots and metrics equipped us with insights into our model's behavior. We learned to appreciate its strengths and identify potential areas for refinement.\n",
    "\n",
    "- **Batch Inference**: The nuances of batch predictions on extensive datasets were explored, alongside methods to seamlessly integrate these predictions back into our primary data.\n",
    "\n",
    "- **Logging Visual Artifacts**: A significant portion of our journey emphasized the importance of logging visual artifacts, like plots, to MLflow. These visuals serve as invaluable references, capturing the state of the model, its performance, and any alterations to the feature set that might sway the model's performance metrics.\n",
    "\n",
    "By the end of this guide, you should possess a robust understanding of a well-structured machine learning workflow. This foundation not only empowers you to craft effective models but also ensures that each step, from data wrangling to predictions, is transparent, reproducible, and efficient.\n",
    "\n",
    "We're grateful you accompanied us on this comprehensive journey. The practices and insights gleaned will undoubtedly be pivotal in all your future machine learning endeavors!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
